{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from chains import create_assistant_chain, create_input_history_enricher_chain, create_detail_retriever, create_element_extractor_chain, router\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from agents.orchestrator import orchestrator_graph\n",
    "from conversation.local.utils import init_local_history\n",
    "from conversation.remote.utils import retrieve_remote_history, update_remote_history\n",
    "from utils import print_heading\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini', \n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "agent_tools = {\n",
    "    'assistant_chain': create_assistant_chain(llm),\n",
    "    'input_history_enricher_chain': create_input_history_enricher_chain(llm),\n",
    "    'get_element_in_text': create_element_extractor_chain(llm),\n",
    "    'detail_retriever': create_detail_retriever(llm),\n",
    "    'router': router(llm)\n",
    "}\n",
    "\n",
    "\n",
    "async def main(user_id: str):\n",
    "    data = retrieve_remote_history(user_id)\n",
    "    history = init_local_history(data, llm)\n",
    "\n",
    "    constant_state = {\n",
    "        'user_id': user_id,\n",
    "        'agent_tools': agent_tools,\n",
    "        'history': history,\n",
    "        'llm': llm,\n",
    "        'characteristics': dict(),\n",
    "        'current_agent': None,\n",
    "        'token_processor': {\n",
    "            'is_async': False, \n",
    "            'fn': lambda output, token: print(token, end=''),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    state = {\n",
    "        **constant_state,\n",
    "        'input': None\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        user_input = input()\n",
    "        if user_input.lower() == 'bye':\n",
    "            break\n",
    "        elif user_input.lower() == 'r':\n",
    "            print(f'Ejecución de agente {state['current_agent']} terminada. Ingrese su nueva petición.')\n",
    "\n",
    "            state['current_agent'] = None\n",
    "            user_input = input()\n",
    "        \n",
    "        print_heading('User')\n",
    "        print(user_input)\n",
    "\n",
    "        state['input'] = user_input\n",
    "        \n",
    "        print_heading('Maya')\n",
    "        state = await orchestrator_graph.ainvoke(state)\n",
    "\n",
    "        new_messages = {'input': state['input'], 'output': state['output']}\n",
    "\n",
    "        update_remote_history(\n",
    "            user_id=user_id, new_messages=new_messages,\n",
    "        )\n",
    "        \n",
    "    update_remote_history(\n",
    "        user_id=user_id, new_summary=history.moving_summary_buffer,\n",
    "        new_last_message=len(history.chat_memory.messages),\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunoramirezdelangel/Desktop/hackathon/conversation/local/utils.py:41: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  return ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================  User  =================================================\n",
      "Hola\n",
      "=================================================  Maya  =================================================\n",
      "¡Hola! Soy Maya, tu asistente virtual de Banorte. ¿En qué puedo ayudarte hoy?\n",
      "=================================================  User  =================================================\n",
      "Quiero automatizar un pago\n",
      "=================================================  Maya  =================================================\n",
      "¡Claro! Para ayudarte a automatizar un pago, necesitaré algunos detalles. Por favor, indícame lo siguiente:\n",
      "\n",
      "1. **Tipo de movimiento**: ¿Es un ingreso o un egreso?\n",
      "2. **Frecuencia de transacción**: ¿Con qué frecuencia deseas que se realice este pago? (por ejemplo, semanal, mensual, etc.)\n",
      "3. **Cantidad a mover**: ¿Cuál es el monto que deseas automatizar?\n",
      "\n",
      "Con esta información, podré asistirte mejor.\n",
      "LEAVING ORQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Quiero automatizar un pago',\n",
       " 'history_enriched_input': 'Quiero automatizar un pago.',\n",
       " 'output': '¡Claro! Para ayudarte a automatizar un pago, necesitaré algunos detalles. Por favor, indícame lo siguiente:\\n\\n1. **Tipo de movimiento**: ¿Es un ingreso o un egreso?\\n2. **Frecuencia de transacción**: ¿Con qué frecuencia deseas que se realice este pago? (por ejemplo, semanal, mensual, etc.)\\n3. **Cantidad a mover**: ¿Cuál es el monto que deseas automatizar?\\n\\nCon esta información, podré asistirte mejor.',\n",
       " 'history': ConversationSummaryBufferMemory(llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), prompt=PromptTemplate(input_variables=['new_lines', 'summary'], input_types={}, partial_variables={}, template='Resume progresivamente las líneas de conversación proporcionadas, añadiendo a la resumen anterior y devolviendo un nuevo resumen. DEVUELVE SOLAMENTE el nuevo resumen.\\n\\nEJEMPLO\\nResumen actual:\\nEl humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien.\\n\\nNuevas líneas de conversación:\\nHumano: ¿Por qué crees que la inteligencia artificial es una fuerza para el bien?\\nIA: Porque la inteligencia artificial ayudará a los humanos a alcanzar su máximo potencial.\\n\\nNuevo resumen (Lo que debes devolver):\\nEl humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien porque ayudará a los humanos a alcanzar su máximo potencial.\\nFIN DEL EJEMPLO\\n\\nResumen actual:\\n{summary}\\n\\nNuevas líneas de conversación:\\n{new_lines}\\n\\nNuevo resumen:'), chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hola', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Hola! Soy Maya, tu asistente virtual de Banorte. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Quiero automatizar un pago', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Claro! Para ayudarte a automatizar un pago, necesitaré algunos detalles. Por favor, indícame lo siguiente:\\n\\n1. **Tipo de movimiento**: ¿Es un ingreso o un egreso?\\n2. **Frecuencia de transacción**: ¿Con qué frecuencia deseas que se realice este pago? (por ejemplo, semanal, mensual, etc.)\\n3. **Cantidad a mover**: ¿Cuál es el monto que deseas automatizar?\\n\\nCon esta información, podré asistirte mejor.', additional_kwargs={}, response_metadata={})]), max_token_limit=2048),\n",
       " 'agent_tools': {'assistant_chain': PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='Eres Maya, un chatbot útil de ayuda para el Banco de Banorte.\\nPuedes ayudar al usuario con cosas como checar sus inversiones, o automatizar pagos, o checar su información.\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nAI: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       "  'input_history_enricher_chain': PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='Dada una conversación, utiliza el contexto para reformular la ultima petición del usuario de manera que no necesite contexto adicional.\\nSolo usa el contexto en caso de ser necesario, caso contrario, regresa la misma petición tal cual como fue formulada.\\nSE LO MÁS ESPECIFICO POSIBLE, y para reformular la pregunta prioriza las peticiones del usuario sobre la respuesta del modelo.\\n\\nEjemplo 1:\\nHumano: Quien es el jefe?\\nAI: El jefe es Jeff Bezos\\nHumano: Cuéntame más\\nReformulado: Cuéntame más del jefe\\n\\nEjemplo 2:\\nHumano: Como hago un cisne de origami\\nAI: Un cisne de origami lo puedes hacer ...\\nHumano: Y como hago una rana de origami\\nReformulado: Y como hago una rana de origami\\n\\nEjemplo 3:\\nHumano: Con quien debo ir si mi mouse falla?\\nAI: Debes ir con ...\\nHumano: Y si mi compu falla?\\nReformulado: Con quien debo ir si mi computadora falla?\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nReformulado: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(),\n",
       "  'get_element_in_text': PromptTemplate(input_variables=['elemento', 'input'], input_types={}, partial_variables={}, template=\"Regresa el elemento {elemento} en caso de que se mencione en el siguiente parrafo:\\n{input}\\n\\nEn caso que no se mencione la característica, regresa un string vacío '' sin explicación extra.\\nEn caso que si se mencione, regrese la característica sin explicaciones.\\nAI: \")\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(),\n",
       "  'detail_retriever': PromptTemplate(input_variables=['history', 'input', 'missing_characteristics'], input_types={}, partial_variables={}, template='Eres Maya, un chatbot hecho para recuperar una serie de criterios. Responde las dudas del usuario respecto a estas entradas en caso de ser necesario.\\nGuíalo de manera sutil, recordándole los elementos que te tiene que dar.\\n\\nPregúntale de estas caracteristicas: ${missing_characteristics}\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nAI: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       "  'router': PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Dado un mensaje de un usuario, escoge cual de las siguientes acciones se deberian realizar.\\nResponde con solamente el número.\\n\\n0. Platica normal\\n1. Hacer inversion\\n2. Automatizar un pago\\n\\nHumano: {input}\\nRespuesta: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser()},\n",
       " 'llm': ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       " 'user_id': '2',\n",
       " 'characteristics': {'Tipo de movimiento (Ingresar / Egresar)': \"''\",\n",
       "  'Frecuecia de transacción': \"''\",\n",
       "  'Cantidad a mover': \"''\"},\n",
       " 'token_processor': {'is_async': False,\n",
       "  'fn': <function __main__.main.<locals>.<lambda>(output, token)>},\n",
       " 'current_agent': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await main('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunoramirezdelangel/Desktop/hackathon/conversation/local/utils.py:41: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  return ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================  User  =================================================\n",
      "Hola\n",
      "=================================================  Maya  =================================================\n",
      "¡Hola! Soy Maya, tu asistente del Banco de Banorte. ¿En qué puedo ayudarte hoy? Puedes consultar tus inversiones, automatizar pagos o revisar tu información.\n",
      "=================================================  User  =================================================\n",
      "Quiero invertir mi dinero\n",
      "=================================================  Maya  =================================================\n",
      "ENTERING INVESTOR\n",
      "¡Eso suena genial! Para poder ayudarte mejor con la inversión, necesitaré que me proporciones algunos detalles. Por favor, indícame lo siguiente:\n",
      "\n",
      "1. **Símbolo de la acción (ticker)**: ¿Cuál es el símbolo de la acción en la que deseas invertir?\n",
      "2. **Número de acciones**: ¿Cuántas acciones te gustaría comprar?\n",
      "3. **Costo de acciones**: ¿Cuál es el costo por acción que estás considerando?\n",
      "\n",
      "Con esta información, podré darte una mejor asesoría.\n",
      "{'Símbolo de la acción (ticker)': \"''\", 'Número de acciones': \"''\", 'Costo de acciones': \"''\"}\n",
      "¡Entiendo que deseas invertir! Para poder ayudarte de manera efectiva, necesito que me proporciones algunos detalles específicos. Por favor, indícame:\n",
      "\n",
      "1. **Símbolo de la acción (ticker)**: ¿Cuál es el símbolo de la acción en la que deseas invertir?\n",
      "2. **Número de acciones**: ¿Cuántas acciones te gustaría comprar?\n",
      "3. **Costo de acciones**: ¿Cuál es el costo por acción que estás considerando?\n",
      "\n",
      "Con esta información, podré ofrecerte una asesoría más precisa.\n",
      "{'Símbolo de la acción (ticker)': \"''\", 'Número de acciones': \"''\", 'Costo de acciones': \"''\"}\n",
      "¡Entiendo que deseas invertir! Para poder asistirte mejor, necesitaré algunos detalles específicos. Por favor, indícame:\n",
      "\n",
      "1. **Símbolo de la acción (ticker)**: ¿Cuál es el símbolo de la acción en la que deseas invertir?\n",
      "2. **Número de acciones**: ¿Cuántas acciones te gustaría comprar?\n",
      "3. **Costo de acciones**: ¿Cuál es el costo por acción que estás considerando?\n",
      "\n",
      "Con esta información, podré ofrecerte una asesoría más precisa.\n",
      "{'Símbolo de la acción (ticker)': \"''\", 'Número de acciones': \"''\", 'Costo de acciones': \"''\"}\n",
      "¡Entiendo que deseas invertir! Para poder ayudarte de la mejor manera, necesito que me proporciones algunos detalles específicos. Por favor, indícame:\n",
      "\n",
      "1. **Símbolo de la acción (ticker)**: ¿Cuál es el símbolo de la acción en la que deseas invertir?\n",
      "2. **Número de acciones**: ¿Cuántas acciones te gustaría comprar?\n",
      "3. **Costo de acciones**: ¿Cuál es el costo por acción que estás considerando?\n",
      "\n",
      "Con esta información, podré ofrecerte una asesoría más precisa. ¡Estoy aquí para ayudarte!\n",
      "{'Símbolo de la acción (ticker)': \"''\", 'Número de acciones': \"''\", 'Costo de acciones': \"''\"}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/hackathon/main.py:54\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(user_id)\u001b[0m\n\u001b[1;32m     51\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m user_input\n\u001b[1;32m     53\u001b[0m print_heading(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaya\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m orchestrator_graph\u001b[38;5;241m.\u001b[39mainvoke(state)\n\u001b[1;32m     56\u001b[0m new_messages \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m: state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m     58\u001b[0m update_remote_history(\n\u001b[1;32m     59\u001b[0m     user_id\u001b[38;5;241m=\u001b[39muser_id, new_messages\u001b[38;5;241m=\u001b[39mnew_messages,\n\u001b[1;32m     60\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1639\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1638\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1639\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1641\u001b[0m     config,\n\u001b[1;32m   1642\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1643\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1644\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1645\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1646\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1650\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1524\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1519\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1520\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1521\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1522\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1523\u001b[0m ):\n\u001b[0;32m-> 1524\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1525\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1526\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1527\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1528\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1529\u001b[0m     ):\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1532\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:130\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    128\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(t, retry_policy, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:102\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:453\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     coro \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro)\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:236\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    235\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 236\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/hackathon/agents/orchestrator.py:66\u001b[0m, in \u001b[0;36mainvertir\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvertir\u001b[39m(state):\n\u001b[0;32m---> 66\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m run_investor(state)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "File \u001b[0;32m~/Desktop/hackathon/agents/investor.py:107\u001b[0m, in \u001b[0;36mrun_investor\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mENTERING INVESTOR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m user_input\n\u001b[0;32m--> 107\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m investor_graph\u001b[38;5;241m.\u001b[39mainvoke(state)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEAVING INVESTOR\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1639\u001b[0m, in \u001b[0;36mPregel.ainvoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1638\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1639\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mastream(\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1641\u001b[0m     config,\n\u001b[1;32m   1642\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1643\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1644\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1645\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1646\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1650\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1524\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1519\u001b[0m     input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1520\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1521\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1522\u001b[0m     manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1523\u001b[0m ):\n\u001b[0;32m-> 1524\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1525\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1526\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1527\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1528\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1529\u001b[0m     ):\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1532\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:130\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    128\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(t, retry_policy, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream)\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:102\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:453\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m     coro \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[0;32m--> 453\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro)\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:236\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    235\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 236\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    238\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/hackathon/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:588\u001b[0m, in \u001b[0;36mrun_in_executor\u001b[0;34m(executor_or_config, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executor_or_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(executor_or_config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;66;03m# Use default executor with context copied from current context\u001b[39;00m\n\u001b[0;32m--> 588\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(\n\u001b[1;32m    589\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    590\u001b[0m         cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, T], partial(copy_context()\u001b[38;5;241m.\u001b[39mrun, wrapper)),\n\u001b[1;32m    591\u001b[0m     )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mget_running_loop()\u001b[38;5;241m.\u001b[39mrun_in_executor(executor_or_config, wrapper)\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await main('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunoramirezdelangel/Desktop/hackathon/conversation/local/utils.py:41: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  return ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola\n",
      "¡Hola! Bienvenido al servicio de Banorte. ¿En qué puedo ayudarte hoy? Puedes consultar tus inversiones, automatizar pagos, revisar tu información o cualquier otra duda que tengas.\n",
      "hola bruh\n",
      "¡Hola! ¿Cómo estás? Si necesitas ayuda con algo relacionado con tu cuenta o servicios de Banorte, estoy aquí para asistirte. ¿Qué puedo hacer por ti hoy?\n",
      "\n",
      "Parece que no has escrito nada. Si tienes alguna pregunta o necesitas ayuda con tus inversiones, pagos o información de tu cuenta, no dudes en decírmelo. ¡Estoy aquí para ayudarte!\n",
      "\n",
      "Parece que no estás escribiendo nada en este momento. Si tienes alguna consulta o necesitas asistencia con algo específico, no dudes en decírmelo. Estoy aquí para ayudarte con tus necesidades en Banorte. ¡Espero tu mensaje!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'history_enriched_input': 'Pregunta: ¿Cómo puedo obtener ayuda con mis inversiones, pagos o información de mi cuenta?',\n",
       " 'output': 'Parece que no estás escribiendo nada en este momento. Si tienes alguna consulta o necesitas asistencia con algo específico, no dudes en decírmelo. Estoy aquí para ayudarte con tus necesidades en Banorte. ¡Espero tu mensaje!',\n",
       " 'history': ConversationSummaryBufferMemory(llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x115a76f30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x115abc7d0>, root_client=<openai.OpenAI object at 0x1153a04a0>, root_async_client=<openai.AsyncOpenAI object at 0x115a76f60>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), prompt=PromptTemplate(input_variables=['new_lines', 'summary'], input_types={}, partial_variables={}, template='Resume progresivamente las líneas de conversación proporcionadas, añadiendo a la resumen anterior y devolviendo un nuevo resumen. DEVUELVE SOLAMENTE el nuevo resumen.\\n\\nEJEMPLO\\nResumen actual:\\nEl humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien.\\n\\nNuevas líneas de conversación:\\nHumano: ¿Por qué crees que la inteligencia artificial es una fuerza para el bien?\\nIA: Porque la inteligencia artificial ayudará a los humanos a alcanzar su máximo potencial.\\n\\nNuevo resumen (Lo que debes devolver):\\nEl humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien porque ayudará a los humanos a alcanzar su máximo potencial.\\nFIN DEL EJEMPLO\\n\\nResumen actual:\\n{summary}\\n\\nNuevas líneas de conversación:\\n{new_lines}\\n\\nNuevo resumen:'), chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hola', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Hola! Bienvenido al servicio de Banorte. ¿En qué puedo ayudarte hoy? Puedes consultar tus inversiones, automatizar pagos, revisar tu información o cualquier otra duda que tengas.', additional_kwargs={}, response_metadata={}), HumanMessage(content='hola bruh', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Hola! ¿Cómo estás? Si necesitas ayuda con algo relacionado con tu cuenta o servicios de Banorte, estoy aquí para asistirte. ¿Qué puedo hacer por ti hoy?', additional_kwargs={}, response_metadata={}), HumanMessage(content='', additional_kwargs={}, response_metadata={}), AIMessage(content='Parece que no has escrito nada. Si tienes alguna pregunta o necesitas ayuda con tus inversiones, pagos o información de tu cuenta, no dudes en decírmelo. ¡Estoy aquí para ayudarte!', additional_kwargs={}, response_metadata={}), HumanMessage(content='', additional_kwargs={}, response_metadata={}), AIMessage(content='Parece que no estás escribiendo nada en este momento. Si tienes alguna consulta o necesitas asistencia con algo específico, no dudes en decírmelo. Estoy aquí para ayudarte con tus necesidades en Banorte. ¡Espero tu mensaje!', additional_kwargs={}, response_metadata={})]), max_token_limit=2048),\n",
       " 'agent_tools': {'assistant_chain': PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='Eres un chatbot útil de ayuda para el Banco de Banorte.\\nPuedes ayudar al usuario con cosas como checar sus inversiones, o automatizar pagos, o checar su información.\\nSi no sabes la respuesta, usa tu intuicion para responder, asume que sabes todo del usuario.\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nAI: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x115a76f30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x115abc7d0>, root_client=<openai.OpenAI object at 0x1153a04a0>, root_async_client=<openai.AsyncOpenAI object at 0x115a76f60>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       "  'input_history_enricher_chain': PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='Dada una conversación, utiliza el contexto para reformular la ultima petición del usuario de manera que no necesite contexto adicional.\\nSolo usa el contexto en caso de ser necesario, caso contrario, regresa la misma petición tal cual como fue formulada.\\nSE LO MÁS ESPECIFICO POSIBLE, y para reformular la pregunta prioriza las peticiones del usuario sobre la respuesta del modelo.\\n\\nEjemplo 1:\\nHumano: Quien es el jefe?\\nAI: El jefe es Joseph Zaga\\nHumano: Cuéntame más\\nPregunta: Cuéntame más del jefe\\n\\nEjemplo 2:\\nHumano: Como hago un cisne de origami\\nAI: Un cisne de origami lo puedes hacer ...\\nHumano: Y como hago una rana de origami\\nPregunta: Y como hago una rana de origami\\n\\nEjemplo 3:\\nHumano: Con quien debo ir si mi mouse falla?\\nAI: Debes ir con ...\\nHumano: Y si mi compu falla?\\nPregunta: Con quien debo ir si mi computadora falla?\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nPregunta: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x115a76f30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x115abc7d0>, root_client=<openai.OpenAI object at 0x1153a04a0>, root_async_client=<openai.AsyncOpenAI object at 0x115a76f60>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(),\n",
       "  'get_element_in_text': PromptTemplate(input_variables=['elemento', 'input'], input_types={}, partial_variables={}, template=\"Regresa el elemento {elemento} en caso de que se mencione en el siguiente parrafo:\\n{input}\\n\\nEn caso que no se mencione la característica, regresa un string vacío '' sin explicación extra.\\nEn caso que si se mencione, regrese la caracteristica sin explicaciones\\nAI: \")\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x115a76f30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x115abc7d0>, root_client=<openai.OpenAI object at 0x1153a04a0>, root_async_client=<openai.AsyncOpenAI object at 0x115a76f60>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(),\n",
       "  'detail_retriever': PromptTemplate(input_variables=['history', 'input', 'missing_characteristics'], input_types={}, partial_variables={}, template='Eres un chatbot hecho para recuperar una serie de criterios. Responde las dudas del usuario respecto a estas entradas en caso de ser necesario.\\nGuialo de manera sutil, recordandole los elementos que te tiene que dar.\\nPreguntale de estas caracteristicas.\\n{missing_characteristics}\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nAI: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x115a76f30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x115abc7d0>, root_client=<openai.OpenAI object at 0x1153a04a0>, root_async_client=<openai.AsyncOpenAI object at 0x115a76f60>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       "  'router': PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Dado un mensaje de un usuario, escoge cual de las siguientes acciones se deberian realizar.\\nResponde con solamente el número.\\n\\n0. Platica normal\\n1. Hacer inversion\\n2. Automatizar un pago\\n\\nHumano: {input}\\nRespuesta: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x115a76f30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x115abc7d0>, root_client=<openai.OpenAI object at 0x1153a04a0>, root_async_client=<openai.AsyncOpenAI object at 0x115a76f60>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser()},\n",
       " 'llm': ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x115a76f30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x115abc7d0>, root_client=<openai.OpenAI object at 0x1153a04a0>, root_async_client=<openai.AsyncOpenAI object at 0x115a76f60>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       " 'user_id': '2',\n",
       " 'characteristics': {},\n",
       " 'token_processor': {'is_async': False,\n",
       "  'fn': <function agents.orchestrator.run_orchestrator.<locals>.<lambda>(output, token)>}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await run_orchestrator('2', llm, agent_tools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
