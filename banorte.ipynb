{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from chains import create_assistant_chain, create_input_history_enricher_chain, create_detail_retriever, create_element_extractor_chain, router\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from agents.orchestrator import orchestrator_graph\n",
    "from conversation.local.utils import init_local_history\n",
    "from conversation.remote.utils import retrieve_remote_history, update_remote_history\n",
    "from utils import print_heading\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4o-mini', \n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")\n",
    "\n",
    "agent_tools = {\n",
    "    'assistant_chain': create_assistant_chain(llm),\n",
    "    'input_history_enricher_chain': create_input_history_enricher_chain(llm),\n",
    "    'get_element_in_text': create_element_extractor_chain(llm),\n",
    "    'detail_retriever': create_detail_retriever(llm),\n",
    "    'router': router(llm)\n",
    "}\n",
    "\n",
    "\n",
    "async def main(user_id: str):\n",
    "    data = retrieve_remote_history(user_id)\n",
    "    history = init_local_history(data, llm)\n",
    "\n",
    "    constant_state = {\n",
    "        'user_id': user_id,\n",
    "        'agent_tools': agent_tools,\n",
    "        'history': history,\n",
    "        'llm': llm,\n",
    "        'characteristics': dict(),\n",
    "        'current_agent': None,\n",
    "        'token_processor': {\n",
    "            'is_async': False, \n",
    "            'fn': lambda output, token: print(token, end=''),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    state = {\n",
    "        **constant_state,\n",
    "        'input': None\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        user_input = input()\n",
    "        if user_input.lower() == 'bye':\n",
    "            break\n",
    "        elif user_input.lower() == 'r':\n",
    "            print(f'Ejecución de agente {state['current_agent']} terminada. Ingrese su nueva petición.')\n",
    "\n",
    "            state['current_agent'] = None\n",
    "            user_input = input()\n",
    "        \n",
    "        print_heading('User')\n",
    "        print(user_input)\n",
    "\n",
    "        state['input'] = user_input\n",
    "        \n",
    "        print_heading('Maya')\n",
    "        state = await orchestrator_graph.ainvoke(state)\n",
    "\n",
    "        new_messages = {'input': state['input'], 'output': state['output']}\n",
    "\n",
    "        update_remote_history(\n",
    "            user_id=user_id, new_messages=new_messages,\n",
    "        )\n",
    "        \n",
    "    update_remote_history(\n",
    "        user_id=user_id, new_summary=history.moving_summary_buffer,\n",
    "        new_last_message=len(history.chat_memory.messages),\n",
    "    )\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brunoramirezdelangel/Desktop/hackathon/conversation/local/utils.py:41: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  return ConversationSummaryBufferMemory(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================  User  =================================================\n",
      "Hola\n",
      "=================================================  Maya  =================================================\n",
      "¡Hola! Soy Maya, tu asistente virtual de Banorte. ¿En qué puedo ayudarte hoy?\n",
      "=================================================  User  =================================================\n",
      "Quiero automatizar un pago\n",
      "=================================================  Maya  =================================================\n",
      "¡Claro! Para ayudarte a automatizar un pago, necesitaré algunos detalles. Por favor, indícame lo siguiente:\n",
      "\n",
      "1. **Tipo de movimiento**: ¿Es un ingreso o un egreso?\n",
      "2. **Frecuencia de transacción**: ¿Con qué frecuencia deseas que se realice este pago? (por ejemplo, semanal, mensual, etc.)\n",
      "3. **Cantidad a mover**: ¿Cuál es el monto que deseas automatizar?\n",
      "\n",
      "Con esta información, podré asistirte mejor.\n",
      "LEAVING ORQ\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Quiero automatizar un pago',\n",
       " 'history_enriched_input': 'Quiero automatizar un pago.',\n",
       " 'output': '¡Claro! Para ayudarte a automatizar un pago, necesitaré algunos detalles. Por favor, indícame lo siguiente:\\n\\n1. **Tipo de movimiento**: ¿Es un ingreso o un egreso?\\n2. **Frecuencia de transacción**: ¿Con qué frecuencia deseas que se realice este pago? (por ejemplo, semanal, mensual, etc.)\\n3. **Cantidad a mover**: ¿Cuál es el monto que deseas automatizar?\\n\\nCon esta información, podré asistirte mejor.',\n",
       " 'history': ConversationSummaryBufferMemory(llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), prompt=PromptTemplate(input_variables=['new_lines', 'summary'], input_types={}, partial_variables={}, template='Resume progresivamente las líneas de conversación proporcionadas, añadiendo a la resumen anterior y devolviendo un nuevo resumen. DEVUELVE SOLAMENTE el nuevo resumen.\\n\\nEJEMPLO\\nResumen actual:\\nEl humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien.\\n\\nNuevas líneas de conversación:\\nHumano: ¿Por qué crees que la inteligencia artificial es una fuerza para el bien?\\nIA: Porque la inteligencia artificial ayudará a los humanos a alcanzar su máximo potencial.\\n\\nNuevo resumen (Lo que debes devolver):\\nEl humano pregunta qué piensa la IA sobre la inteligencia artificial. La IA piensa que la inteligencia artificial es una fuerza para el bien porque ayudará a los humanos a alcanzar su máximo potencial.\\nFIN DEL EJEMPLO\\n\\nResumen actual:\\n{summary}\\n\\nNuevas líneas de conversación:\\n{new_lines}\\n\\nNuevo resumen:'), chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Hola', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Hola! Soy Maya, tu asistente virtual de Banorte. ¿En qué puedo ayudarte hoy?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Quiero automatizar un pago', additional_kwargs={}, response_metadata={}), AIMessage(content='¡Claro! Para ayudarte a automatizar un pago, necesitaré algunos detalles. Por favor, indícame lo siguiente:\\n\\n1. **Tipo de movimiento**: ¿Es un ingreso o un egreso?\\n2. **Frecuencia de transacción**: ¿Con qué frecuencia deseas que se realice este pago? (por ejemplo, semanal, mensual, etc.)\\n3. **Cantidad a mover**: ¿Cuál es el monto que deseas automatizar?\\n\\nCon esta información, podré asistirte mejor.', additional_kwargs={}, response_metadata={})]), max_token_limit=2048),\n",
       " 'agent_tools': {'assistant_chain': PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='Eres Maya, un chatbot útil de ayuda para el Banco de Banorte.\\nPuedes ayudar al usuario con cosas como checar sus inversiones, o automatizar pagos, o checar su información.\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nAI: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       "  'input_history_enricher_chain': PromptTemplate(input_variables=['history', 'input'], input_types={}, partial_variables={}, template='Dada una conversación, utiliza el contexto para reformular la ultima petición del usuario de manera que no necesite contexto adicional.\\nSolo usa el contexto en caso de ser necesario, caso contrario, regresa la misma petición tal cual como fue formulada.\\nSE LO MÁS ESPECIFICO POSIBLE, y para reformular la pregunta prioriza las peticiones del usuario sobre la respuesta del modelo.\\n\\nEjemplo 1:\\nHumano: Quien es el jefe?\\nAI: El jefe es Jeff Bezos\\nHumano: Cuéntame más\\nReformulado: Cuéntame más del jefe\\n\\nEjemplo 2:\\nHumano: Como hago un cisne de origami\\nAI: Un cisne de origami lo puedes hacer ...\\nHumano: Y como hago una rana de origami\\nReformulado: Y como hago una rana de origami\\n\\nEjemplo 3:\\nHumano: Con quien debo ir si mi mouse falla?\\nAI: Debes ir con ...\\nHumano: Y si mi compu falla?\\nReformulado: Con quien debo ir si mi computadora falla?\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nReformulado: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(),\n",
       "  'get_element_in_text': PromptTemplate(input_variables=['elemento', 'input'], input_types={}, partial_variables={}, template=\"Regresa el elemento {elemento} en caso de que se mencione en el siguiente parrafo:\\n{input}\\n\\nEn caso que no se mencione la característica, regresa un string vacío '' sin explicación extra.\\nEn caso que si se mencione, regrese la característica sin explicaciones.\\nAI: \")\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser(),\n",
       "  'detail_retriever': PromptTemplate(input_variables=['history', 'input', 'missing_characteristics'], input_types={}, partial_variables={}, template='Eres Maya, un chatbot hecho para recuperar una serie de criterios. Responde las dudas del usuario respecto a estas entradas en caso de ser necesario.\\nGuíalo de manera sutil, recordándole los elementos que te tiene que dar.\\n\\nPregúntale de estas caracteristicas: ${missing_characteristics}\\n\\nConversación actual:\\n{history}\\nHumano: {input}\\nAI: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       "  'router': PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='Dado un mensaje de un usuario, escoge cual de las siguientes acciones se deberian realizar.\\nResponde con solamente el número.\\n\\n0. Platica normal\\n1. Hacer inversion\\n2. Automatizar un pago\\n\\nHumano: {input}\\nRespuesta: ')\n",
       "  | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "  | StrOutputParser()},\n",
       " 'llm': ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x1210956a0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x12071f6e0>, root_client=<openai.OpenAI object at 0x117d82ed0>, root_async_client=<openai.AsyncOpenAI object at 0x121095700>, model_name='gpt-4o-mini', model_kwargs={}, openai_api_key=SecretStr('**********')),\n",
       " 'user_id': '2',\n",
       " 'characteristics': {'Tipo de movimiento (Ingresar / Egresar)': \"''\",\n",
       "  'Frecuecia de transacción': \"''\",\n",
       "  'Cantidad a mover': \"''\"},\n",
       " 'token_processor': {'is_async': False,\n",
       "  'fn': <function __main__.main.<locals>.<lambda>(output, token)>},\n",
       " 'current_agent': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await main('2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
